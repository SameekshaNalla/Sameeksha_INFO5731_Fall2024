{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SameekshaNalla/Sameeksha_INFO5731_Fall2024/blob/main/Nalla_Sameeksha_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "\n",
        "\n",
        "# **INFO5731 Assignment 3**\n",
        "\n",
        "In this assignment, we will delve into various aspects of natural language processing (NLP) and text analysis. The tasks are designed to deepen your understanding of key NLP concepts and techniques, as well as to provide hands-on experience with practical applications.\n",
        "\n",
        "Through these tasks, you'll gain practical experience in NLP techniques such as N-gram analysis, TF-IDF, word embedding model creation, and sentiment analysis dataset creation.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: See Canvas\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "## Question 1 (30 points)\n",
        "\n",
        "**Understand N-gram**\n",
        "\n",
        "Write a python program to conduct N-gram analysis based on the dataset in your assignment two. You need to write codes from **scratch instead of using any pre-existing libraries** to do so:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3) and (N=2).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the formula  count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the noun phrases and calculate the relative\n",
        "probabilities of each review in terms of other reviews (abstracts, or tweets) by using the formula  frequency (noun phrase) / max frequency (noun phrase) on the whole dataset.\n",
        "\n",
        "Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9v8IikDpqrxn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "fa8cab1c-c3ae-4041-d4da-dd7c34f631ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'densho_narrators_clean.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2930140467.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'densho_narrators_clean.csv'"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "# -------------------------------\n",
        "# N-GRAM ANALYSIS (Assignment 3)\n",
        "# Dataset: densho_narrators_clean.csv\n",
        "# Column: display_name\n",
        "# Author: Sameeksha Nalla\n",
        "# -------------------------------\n",
        "\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# 1. LOAD DATA\n",
        "\n",
        "filename = \"densho_narrators_clean.csv\"\n",
        "\n",
        "texts = []\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        if row['display_name'].strip():\n",
        "            texts.append(row['display_name'].lower())\n",
        "\n",
        "print(f\"Loaded {len(texts)} reviews/entries.\")\n",
        "\n",
        "\n",
        "# 2. TOKENIZATION HELPER\n",
        "\n",
        "def tokenize(text):\n",
        "    # remove punctuation and split\n",
        "    text = re.sub(r'[^a-z\\s]', '', text.lower())\n",
        "    return [w for w in text.split() if w]\n",
        "\n",
        "\n",
        "# 3. COMPUTE N-GRAM FREQUENCIES (BIGRAMS + TRIGRAMS)\n",
        "\n",
        "bigram_freq = defaultdict(int)\n",
        "trigram_freq = defaultdict(int)\n",
        "unigram_freq = defaultdict(int)\n",
        "\n",
        "for text in texts:\n",
        "    tokens = tokenize(text)\n",
        "    for i in range(len(tokens)):\n",
        "        unigram_freq[tokens[i]] += 1\n",
        "        if i < len(tokens) - 1:\n",
        "            bigram = (tokens[i], tokens[i + 1])\n",
        "            bigram_freq[bigram] += 1\n",
        "        if i < len(tokens) - 2:\n",
        "            trigram = (tokens[i], tokens[i + 1], tokens[i + 2])\n",
        "            trigram_freq[trigram] += 1\n",
        "\n",
        "print(\"\\nTop 5 Bigrams:\")\n",
        "for bg, cnt in sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "    print(bg, \":\", cnt)\n",
        "\n",
        "print(\"\\nTop 5 Trigrams:\")\n",
        "for tg, cnt in sorted(trigram_freq.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "    print(tg, \":\", cnt)\n",
        "\n",
        "\n",
        "# 4. BIGRAM PROBABILITIES\n",
        "# Formula: P(w2 | w1) = count(w1 w2) / count(w1)\n",
        "\n",
        "\n",
        "bigram_prob = {}\n",
        "for (w1, w2), cnt in bigram_freq.items():\n",
        "    bigram_prob[(w1, w2)] = round(cnt / unigram_freq[w1], 4)\n",
        "\n",
        "print(\"\\nSample Bigram Probabilities:\")\n",
        "for k, v in list(bigram_prob.items())[:5]:\n",
        "    print(f\"P({k[1]}|{k[0]}) = {v}\")\n",
        "\n",
        "\n",
        "# 5. EXTRACT NOUN PHRASES (SIMPLE HEURISTIC)\n",
        "\n",
        "noun_phrases = []\n",
        "for text in texts:\n",
        "    phrases = re.findall(r'([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*)', text.title())\n",
        "    noun_phrases.extend(phrases)\n",
        "\n",
        "# Compute global frequencies of each noun phrase\n",
        "noun_freq = defaultdict(int)\n",
        "for np in noun_phrases:\n",
        "    noun_freq[np] += 1\n",
        "\n",
        "max_freq = max(noun_freq.values()) if noun_freq else 1\n",
        "\n",
        "\n",
        "# 6. RELATIVE PROBABILITY TABLE\n",
        "\n",
        "rows = []\n",
        "for i, text in enumerate(texts):\n",
        "    row_counts = defaultdict(int)\n",
        "    local_phrases = re.findall(r'([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*)', text.title())\n",
        "    for np in local_phrases:\n",
        "        row_counts[np] += 1\n",
        "    # relative probability = freq(np in this review) / max freq(np overall)\n",
        "    rel_probs = {np: round(row_counts[np] / max_freq, 3) for np in noun_freq.keys()}\n",
        "    rel_probs['review_id'] = f\"Review_{i+1}\"\n",
        "    rows.append(rel_probs)\n",
        "\n",
        "# Create pandas DataFrame (rows=reviews, cols=noun phrases)\n",
        "df = pd.DataFrame(rows).fillna(0).set_index('review_id')\n",
        "\n",
        "print(\"\\nRelative Probability Table (sample):\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# 7. SAVE RESULTS\n",
        "\n",
        "df.to_csv(\"noun_phrase_relative_probabilities.csv\", index=True)\n",
        "print(\"\\nSaved table as 'noun_phrase_relative_probabilities.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "## Question 2 (25 points)\n",
        "\n",
        "**Undersand TF-IDF and Document representation**\n",
        "\n",
        "Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program:\n",
        "\n",
        "(1) To build the documents-terms weights (tf * idf) matrix.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using cosine similarity.\n",
        "\n",
        "Note: You need to write codes from scratch instead of using any **pre-existing libraries** to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjN0iysvo9-n"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "# ----------------------------------------------\n",
        "# TF-IDF & Cosine Similarity (Assignment 3)\n",
        "# Dataset: densho_narrators_clean.csv\n",
        "# Column: display_name\n",
        "# Author: Sameeksha Nalla\n",
        "# ----------------------------------------------\n",
        "\n",
        "import csv, re, math\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# 1. LOAD DATA\n",
        "\n",
        "filename = \"densho_narrators_clean.csv\"\n",
        "docs = []\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        if row[\"display_name\"].strip():\n",
        "            docs.append(row[\"display_name\"].lower())\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents.\")\n",
        "\n",
        "\n",
        "# 2. TOKENIZATION HELPER\n",
        "\n",
        "def tokenize(text):\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text.lower())\n",
        "    return [w for w in text.split() if w]\n",
        "\n",
        "tokenized_docs = [tokenize(d) for d in docs]\n",
        "\n",
        "\n",
        "# 3. BUILD VOCABULARY\n",
        "\n",
        "vocab = sorted(set(w for doc in tokenized_docs for w in doc))\n",
        "N = len(tokenized_docs)\n",
        "\n",
        "\n",
        "# 4. COMPUTE TERM FREQUENCY (TF)\n",
        "\n",
        "tf = []  # list of dictionaries, one per document\n",
        "for doc in tokenized_docs:\n",
        "    counts = defaultdict(int)\n",
        "    for w in doc:\n",
        "        counts[w] += 1\n",
        "    total = len(doc)\n",
        "    tf.append({w: counts[w] / total for w in counts})\n",
        "\n",
        "\n",
        "# 5. COMPUTE INVERSE DOCUMENT FREQUENCY (IDF)\n",
        "\n",
        "df = defaultdict(int)\n",
        "for w in vocab:\n",
        "    df[w] = sum(1 for doc in tokenized_docs if w in doc)\n",
        "\n",
        "idf = {w: math.log((N + 1) / (df[w] + 1)) + 1 for w in vocab}\n",
        "\n",
        "# 6. COMPUTE TF-IDF MATRIX\n",
        "\n",
        "tfidf = []\n",
        "for i, doc_tf in enumerate(tf):\n",
        "    weights = {w: doc_tf.get(w, 0) * idf[w] for w in vocab}\n",
        "    tfidf.append(weights)\n",
        "\n",
        "print(\"\\nSample TF-IDF (first doc, top 10 terms):\")\n",
        "for w, val in list(sorted(tfidf[0].items(), key=lambda x: x[1], reverse=True))[:10]:\n",
        "    print(w, \":\", round(val, 4))\n",
        "\n",
        "\n",
        "# 7. COSINE SIMILARITY FUNCTION\n",
        "\n",
        "def cosine_sim(vec1, vec2):\n",
        "    # dot product\n",
        "    dot = sum(vec1[w] * vec2.get(w, 0) for w in vec1)\n",
        "    mag1 = math.sqrt(sum(v * v for v in vec1.values()))\n",
        "    mag2 = math.sqrt(sum(v * v for v in vec2.values()))\n",
        "    return 0 if mag1 == 0 or mag2 == 0 else dot / (mag1 * mag2)\n",
        "\n",
        "\n",
        "# 8. USER QUERY (DESIGN YOUR OWN)\n",
        "\n",
        "query = \"an outstanding movie with a haunting performance and best character development\"\n",
        "q_tokens = tokenize(query)\n",
        "q_counts = defaultdict(int)\n",
        "for w in q_tokens:\n",
        "    q_counts[w] += 1\n",
        "q_total = len(q_tokens)\n",
        "q_tf = {w: q_counts[w] / q_total for w in q_counts}\n",
        "q_tfidf = {w: q_tf[w] * idf.get(w, math.log((N + 1) / 1)) for w in q_tf}\n",
        "\n",
        "\n",
        "# 9. RANK DOCUMENTS BY COSINE SIMILARITY\n",
        "\n",
        "scores = []\n",
        "for i, doc_vec in enumerate(tfidf):\n",
        "    sim = cosine_sim(doc_vec, q_tfidf)\n",
        "    scores.append((i, sim))\n",
        "\n",
        "ranked = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nTop 5 documents for query:\")\n",
        "for idx, score in ranked[:5]:\n",
        "    print(f\"Doc {idx+1} | Score = {round(score,4)} | Text = {docs[idx][:60]}...\")\n",
        "\n",
        "\n",
        "# 10. OPTIONAL: SAVE MATRIX & SCORES\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(tfidf).to_csv(\"tfidf_matrix.csv\", index=False)\n",
        "pd.DataFrame(ranked, columns=[\"doc_id\", \"similarity\"]).to_csv(\"cosine_similarity_scores.csv\", index=False)\n",
        "print(\"\\nSaved results to 'tfidf_matrix.csv' and 'cosine_similarity_scores.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "## Question 3 (25 points)\n",
        "\n",
        "**Create your own word embedding model**\n",
        "\n",
        "Use the data you collected for assignment 2 to build a word embedding model:\n",
        "\n",
        "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit or Fine tune bert model).\n",
        "\n",
        "(2) Visualize the embeddings using PCA or t-SNE in 2D. Create a scatter plot of at least 20 words and show how similar words cluster together.\n",
        "\n",
        "(3) Calculate the cosine similarity between a few pairs of words to see if the model captures semantic similarity accurately.\n",
        "\n",
        "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
        "\n",
        "Reference: https://jaketae.github.io/study/word2vec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEGOv482OX4D"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eczZgyAoo05Q"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# Question 3 – Word Embedding Model from Scratch\n",
        "# Dataset: densho_narrators_clean.csv\n",
        "# Column: display_name\n",
        "# ----------------------------------------------\n",
        "\n",
        "import csv, re, math, random, time\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# LOAD DATA\n",
        "file = \"densho_narrators_clean.csv\"\n",
        "text_col = \"display_name\"\n",
        "docs = []\n",
        "with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    for row in reader:\n",
        "        txt = (row.get(text_col) or \"\").strip()\n",
        "        if txt:\n",
        "            docs.append(txt)\n",
        "\n",
        "def tokenize(t):\n",
        "    t = re.sub(r\"[^a-z\\s]\", \" \", t.lower())\n",
        "    return [w for w in t.split() if w]\n",
        "\n",
        "sentences = [tokenize(d) for d in docs if tokenize(d)]\n",
        "print(\"Loaded\", len(sentences), \"sentences\")\n",
        "\n",
        "#  BUILD VOCAB\n",
        "word_counts = Counter(w for s in sentences for w in s)\n",
        "vocab = sorted(word_counts)\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "V = len(vocab)\n",
        "print(\"Vocab size:\", V)\n",
        "\n",
        "#  NEGATIVE SAMPLING TABLE\n",
        "pow_freq = np.array([word_counts[w]**0.75 for w in vocab])\n",
        "neg_dist = pow_freq / pow_freq.sum()\n",
        "neg_table = np.random.choice(np.arange(V), size=int(1e6), p=neg_dist)\n",
        "def sample_neg(k): return neg_table[np.random.randint(0, len(neg_table), size=k)]\n",
        "\n",
        "# INIT PARAMETERS\n",
        "D = 300\n",
        "W_in = (np.random.rand(V,D)-0.5)/D\n",
        "W_out = (np.random.rand(V,D)-0.5)/D\n",
        "window, neg_k, epochs = 2, 5, 2\n",
        "lr = 0.025\n",
        "\n",
        "def sigmoid(x): return 1/(1+np.exp(-x))\n",
        "\n",
        "# TRAINING (Skip-gram + Negative Sampling)\n",
        "for ep in range(epochs):\n",
        "    start = time.time()\n",
        "    random.shuffle(sentences)\n",
        "    for s in sentences:\n",
        "        for i,c in enumerate(s):\n",
        "            center = word2idx[c]\n",
        "            v_c = W_in[center]\n",
        "            for j in range(max(0,i-window), min(len(s),i+window+1)):\n",
        "                if i==j: continue\n",
        "                context = word2idx[s[j]]\n",
        "                v_o = W_out[context]\n",
        "                # positive\n",
        "                score = sigmoid(np.dot(v_c,v_o))\n",
        "                grad = (1-score)\n",
        "                W_in[center]  += lr*grad*v_o\n",
        "                W_out[context]+= lr*grad*v_c\n",
        "                # negatives\n",
        "                for n in sample_neg(neg_k):\n",
        "                    v_n = W_out[n]\n",
        "                    score_n = sigmoid(-np.dot(v_c,v_n))\n",
        "                    grad_n = (1-score_n)\n",
        "                    W_in[center]  += lr*(-grad_n)*v_n\n",
        "                    W_out[n]      += lr*(-grad_n)*v_c\n",
        "    print(f\"Epoch {ep+1} done in {time.time()-start:.2f}s\")\n",
        "\n",
        "emb = W_in.copy()\n",
        "\n",
        "# PCA (2-D) USING SVD\n",
        "words_show = [w for w,_ in word_counts.most_common(30)]\n",
        "X = emb[[word2idx[w] for w in words_show]]\n",
        "Xc = X - X.mean(0)\n",
        "U,S,VT = np.linalg.svd(Xc, full_matrices=False)\n",
        "proj = Xc @ VT[:2].T\n",
        "\n",
        "plt.figure(figsize=(9,7))\n",
        "plt.scatter(proj[:,0], proj[:,1])\n",
        "for i,w in enumerate(words_show):\n",
        "    plt.annotate(w,(proj[i,0],proj[i,1]),fontsize=9)\n",
        "plt.title(\"Word Embeddings (300-D → 2-D PCA)\")\n",
        "plt.show()\n",
        "\n",
        "# COSINE SIMILARITY EXAMPLES\n",
        "def cos(a,b):\n",
        "    a,b = emb[word2idx[a]], emb[word2idx[b]]\n",
        "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)+1e-9)\n",
        "\n",
        "pairs=[(\"george\",\"mary\"),(\"family\",\"home\"),(\"man\",\"woman\")]\n",
        "for a,b in pairs:\n",
        "    if a in word2idx and b in word2idx:\n",
        "        print(f\"cos({a},{b})={cos(a,b):.4f}\")\n",
        "    else:\n",
        "        print(f\"Pair missing: {a},{b}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDoVp3aYoU8F"
      },
      "source": [
        "## Question 4 (20 Points)\n",
        "\n",
        "**Create your own training and evaluation dataset for an NLP task.**\n",
        "\n",
        " **You don't need to write program for this question!**\n",
        "\n",
        " For example, if you collected a movie review or a product review data, then you can do the following steps:\n",
        "\n",
        "*   Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral).\n",
        "\n",
        "*   Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew.\n",
        "\n",
        "*   This datset will be used for assignment four: sentiment analysis and text classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-JtOozpvZ-3"
      },
      "source": [
        "\n",
        "\n",
        "1.   Which NLP Task you would like perform on your selected dataset\n",
        "(NER, Summarization, Sentiment Analysis, Text classficication)\n",
        "2.  Explain your labeling Schema you have used and mention those labels\n",
        "\n",
        "3.  You can take AI assistance for labeling the data only.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyK54UY6ompS"
      },
      "outputs": [],
      "source": [
        "# The GitHub link of final csv file\n",
        "\n",
        "\n",
        "# Link: https://github.com/SameekshaNalla/nlp-sentiment-dataset/blob/main/MovieReviews_Dataset_Labeled.csv\n",
        "\n",
        "# 1. The dataset is movie reviews dataset. I would like to perform Sentiment Analysis on the dataset.\n",
        "\n",
        "# 2. The dataset uses a three-class labeling schema to identify the sentiment of each movie review:\n",
        "\n",
        "#Positive: Reviews that express favorable opinions or satisfaction with the movie.\n",
        "#Negative: Reviews that show dissatisfaction or criticism of the movie.\n",
        "#Neutral: Reviews that provide balanced or factual feedback without strong emotion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment by filling this survey link. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNXlsbrirHRo"
      },
      "outputs": [],
      "source": [
        "# Type your answer"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}