{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SameekshaNalla/Sameeksha_INFO5731_Fall2024/blob/main/nalla_sameeksha_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "outputId": "ba6b36d7-b32f-4131-e612-fe0857508030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetched 1009 narrators across 41 pages.\n",
            "Saved: /Users/19452/Downloads/densho_narrators.csv\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Q1 â€“ Collect all information of the narrators in the Densho Digital Repository\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "from urllib.request import Request, urlopen\n",
        "from urllib.error import HTTPError, URLError\n",
        "\n",
        "API_ROOT = \"https://ddr.densho.org/api/0.2/narrator/\"\n",
        "OUT_PATH = \"/Users/19452/Downloads/densho_narrators.csv\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"Accept\": \"application/json\",\n",
        "    \"User-Agent\": \"INFO5731-Assignment/1.0\"\n",
        "}\n",
        "\n",
        "FIELDS = [\n",
        "    \"id\", \"display_name\", \"first_name\", \"last_name\",\n",
        "    \"birth_location\", \"b_date\", \"d_date\", \"gender\", \"generation\",\n",
        "    \"ethnicity\", \"nationality\", \"bio\", \"created\", \"modified\", \"resource_uri\"\n",
        "]\n",
        "\n",
        "RETRIABLE = {429, 500, 502, 503, 504}\n",
        "\n",
        "\n",
        "def fetch_json(url: str, retries: int = 5, base_delay: float = 1.0) -> dict:\n",
        "    delay = base_delay\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            req = Request(url, headers=HEADERS)\n",
        "            with urlopen(req, timeout=30) as resp:\n",
        "                return json.loads(resp.read().decode(\"utf-8\"))\n",
        "        except HTTPError as e:\n",
        "            if e.code in RETRIABLE:\n",
        "                time.sleep(delay)\n",
        "                delay *= 2\n",
        "                continue\n",
        "            raise\n",
        "        except URLError:\n",
        "            time.sleep(delay)\n",
        "            delay *= 2\n",
        "    raise RuntimeError(f\"Request failed after {retries} attempts: {url}\")\n",
        "\n",
        "\n",
        "def normalize(rec: dict) -> dict:\n",
        "    row = {k: \"\" for k in FIELDS}\n",
        "    row[\"id\"] = rec.get(\"id\") or rec.get(\"pk\") or \"\"\n",
        "    row[\"display_name\"] = (rec.get(\"display_name\") or \"\").strip()\n",
        "    row[\"first_name\"] = (rec.get(\"first_name\") or \"\").strip()\n",
        "    row[\"last_name\"] = (rec.get(\"last_name\") or \"\").strip()\n",
        "    row[\"birth_location\"] = (rec.get(\"birth_location\") or \"\").strip()\n",
        "    row[\"b_date\"] = (rec.get(\"b_date\") or rec.get(\"birth_date\") or \"\").strip()\n",
        "    row[\"d_date\"] = (rec.get(\"d_date\") or rec.get(\"death_date\") or \"\").strip()\n",
        "    row[\"gender\"] = (rec.get(\"gender\") or \"\").strip()\n",
        "    row[\"generation\"] = (rec.get(\"generation\") or \"\").strip()\n",
        "    row[\"ethnicity\"] = (rec.get(\"ethnicity\") or \"\").strip()\n",
        "    row[\"nationality\"] = (rec.get(\"nationality\") or \"\").strip()\n",
        "\n",
        "    bio = rec.get(\"bio\") or rec.get(\"biography\") or rec.get(\"summary\") or rec.get(\"description\") or \"\"\n",
        "    row[\"bio\"] = bio[:6000]  # keep CSV manageable\n",
        "\n",
        "    row[\"created\"] = (rec.get(\"created\") or \"\").strip()\n",
        "    row[\"modified\"] = (rec.get(\"modified\") or \"\").strip()\n",
        "\n",
        "    links = rec.get(\"links\") or {}\n",
        "    row[\"resource_uri\"] = links.get(\"html\") or links.get(\"json\") or rec.get(\"resource_uri\") or \"\"\n",
        "    return row\n",
        "\n",
        "\n",
        "def page_items_and_next(payload: dict):\n",
        "    # Support both possible payload styles\n",
        "    items = payload.get(\"objects\") or payload.get(\"results\") or []\n",
        "    nxt = payload.get(\"next_api\") or payload.get(\"next\")\n",
        "    return items, nxt\n",
        "\n",
        "\n",
        "def collect_all() -> list[dict]:\n",
        "    url = API_ROOT\n",
        "    rows: list[dict] = []\n",
        "    seen = set()\n",
        "    pages = 0\n",
        "\n",
        "    while url:\n",
        "        data = fetch_json(url)\n",
        "        items, url = page_items_and_next(data)\n",
        "        for rec in items:\n",
        "            nid = rec.get(\"id\") or rec.get(\"pk\")\n",
        "            if nid in seen:\n",
        "                continue\n",
        "            seen.add(nid)\n",
        "            rows.append(normalize(rec))\n",
        "        pages += 1\n",
        "        time.sleep(0.4)  # be polite\n",
        "\n",
        "    print(f\"Fetched {len(rows)} narrators across {pages} pages.\")\n",
        "    return rows\n",
        "\n",
        "\n",
        "def save_csv(rows: list[dict], path: str):\n",
        "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=FIELDS)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "\n",
        "\n",
        "def main():\n",
        "    rows = collect_all()\n",
        "    save_csv(rows, OUT_PATH)\n",
        "    print(f\"Saved: {OUT_PATH}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "outputId": "1a4b9422-705b-41ca-8709-ae3bccbccf89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "bio_no_noise:\n",
            "1. Nisei female Born May 9 1927 in Selleck Washington Spent much of childhood in Beaverton Oregon where father owned a farm Influenced at an early age by parents conversion to Christianity During World War II removed to the Portland Assembly Center Oregon and the Minidoka concentration camp Idaho After the war worked to establish a successful volunteer program to feed the homeless in Seattle Washington\n",
            "2. Nisei male Born June 12 1921 in Seattle Washington Grew up in an area of Seattle with few other Japanese Americans and was attending the University of Washington when Japan bombed Pearl Harbor Removed to the Puyallup Assembly Center Washington and the Minidoka concentration camp Idaho Left Minidoka several times on temporary work leave to work on farms in the area Suffered tragic loss in camp when father got lost outside collecting wood and perished in the elements Volunteered for the army and served in the signal corps eventually working for the Civil Aeronautics Administration and Boeing Also one of the early members of the Seattle Nisei Veterans Committee\n",
            "3. Nisei female Born October 31 1925 in Seattle Washington Family owned and operated a successful grocery store prior to World War II After the bombing of Pearl Harbor removed to the Puyallup Assembly Center Washington and the Minidoka concentration camp Idaho Left camp in 1944 to attend St Mary s School of Nursing in Rochester Minnesota Worked in the medical field in Minnesota and Seattle before eventually pursuing a career in education\n",
            "\n",
            "bio_no_numbers:\n",
            "1. Nisei female Born May in Selleck Washington Spent much of childhood in Beaverton Oregon where father owned a farm Influenced at an early age by parents conversion to Christianity During World War II removed to the Portland Assembly Center Oregon and the Minidoka concentration camp Idaho After the war worked to establish a successful volunteer program to feed the homeless in Seattle Washington\n",
            "2. Nisei male Born June in Seattle Washington Grew up in an area of Seattle with few other Japanese Americans and was attending the University of Washington when Japan bombed Pearl Harbor Removed to the Puyallup Assembly Center Washington and the Minidoka concentration camp Idaho Left Minidoka several times on temporary work leave to work on farms in the area Suffered tragic loss in camp when father got lost outside collecting wood and perished in the elements Volunteered for the army and served in the signal corps eventually working for the Civil Aeronautics Administration and Boeing Also one of the early members of the Seattle Nisei Veterans Committee\n",
            "3. Nisei female Born October in Seattle Washington Family owned and operated a successful grocery store prior to World War II After the bombing of Pearl Harbor removed to the Puyallup Assembly Center Washington and the Minidoka concentration camp Idaho Left camp in to attend St Mary s School of Nursing in Rochester Minnesota Worked in the medical field in Minnesota and Seattle before eventually pursuing a career in education\n",
            "\n",
            "bio_lower:\n",
            "1. nisei female born may in selleck washington spent much of childhood in beaverton oregon where father owned a farm influenced at an early age by parents conversion to christianity during world war ii removed to the portland assembly center oregon and the minidoka concentration camp idaho after the war worked to establish a successful volunteer program to feed the homeless in seattle washington\n",
            "2. nisei male born june in seattle washington grew up in an area of seattle with few other japanese americans and was attending the university of washington when japan bombed pearl harbor removed to the puyallup assembly center washington and the minidoka concentration camp idaho left minidoka several times on temporary work leave to work on farms in the area suffered tragic loss in camp when father got lost outside collecting wood and perished in the elements volunteered for the army and served in the signal corps eventually working for the civil aeronautics administration and boeing also one of the early members of the seattle nisei veterans committee\n",
            "3. nisei female born october in seattle washington family owned and operated a successful grocery store prior to world war ii after the bombing of pearl harbor removed to the puyallup assembly center washington and the minidoka concentration camp idaho left camp in to attend st mary s school of nursing in rochester minnesota worked in the medical field in minnesota and seattle before eventually pursuing a career in education\n",
            "\n",
            "bio_no_stop:\n",
            "1. nisei female born may selleck washington spent much childhood beaverton oregon father owned farm influenced early age parents conversion christianity world war ii removed portland assembly center oregon minidoka concentration camp idaho war worked establish successful volunteer program feed homeless seattle washington\n",
            "2. nisei male born june seattle washington grew area seattle japanese americans attending university washington japan bombed pearl harbor removed puyallup assembly center washington minidoka concentration camp idaho left minidoka several times temporary work leave work farms area suffered tragic loss camp father got lost outside collecting wood perished elements volunteered army served signal corps eventually working civil aeronautics administration boeing also one early members seattle nisei veterans committee\n",
            "3. nisei female born october seattle washington family owned operated successful grocery store prior world war ii bombing pearl harbor removed puyallup assembly center washington minidoka concentration camp idaho left camp attend st mary school nursing rochester minnesota worked medical field minnesota seattle eventually pursuing career education\n",
            "\n",
            "bio_stemmed:\n",
            "1. nisei femal born may selleck washington spent much childhood beaverton oregon father own farm influenc earli age parent convers christian world war ii remov portland assembl center oregon minidoka concentr camp idaho war work establish success volunt program feed homeless seattl washington\n",
            "2. nisei male born june seattl washington grew area seattl japanes american attend univers washington japan bomb pearl harbor remov puyallup assembl center washington minidoka concentr camp idaho left minidoka sever time temporari work leav work farm area suffer tragic loss camp father got lost outsid collect wood perish element volunt armi serv signal corp eventu work civil aeronaut administr boe also one earli member seattl nisei veteran committe\n",
            "3. nisei femal born octob seattl washington famili own oper success groceri store prior world war ii bomb pearl harbor remov puyallup assembl center washington minidoka concentr camp idaho left camp attend st mari school nurs rochest minnesota work medic field minnesota seattl eventu pursu career educ\n",
            "\n",
            "bio_lemma:\n",
            "1. nisei female bear may selleck washington spend much childhood beaverton oregon father own farm influence early age parent conversion christianity world war ii remove portland assembly center oregon minidoka concentration camp idaho war work establish successful volunteer program feed homeless seattle washington\n",
            "2. nisei male bear june seattle washington grow area seattle japanese americans attend university washington japan bomb pearl harbor remove puyallup assembly center washington minidoka concentration camp idaho leave minidoka several time temporary work leave work farm area suffer tragic loss camp father get lose outside collect wood perish element volunteer army serve signal corp eventually work civil aeronautic administration boeing also one early member seattle nisei veterans committee\n",
            "3. nisei female bear october seattle washington family own operate successful grocery store prior world war ii bombing pearl harbor remove puyallup assembly center washington minidoka concentration camp idaho leave camp attend st mary school nursing rochester minnesota work medical field minnesota seattle eventually pursue career education\n",
            "\n",
            "Saved: /Users/19452/Downloads/densho_narrators_clean.csv\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# setup\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "STOP = set(stopwords.words(\"english\"))\n",
        "STEM = PorterStemmer()\n",
        "LEMM = WordNetLemmatizer()\n",
        "\n",
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n",
        "    USE_SPACY = True\n",
        "except Exception:\n",
        "    USE_SPACY = False\n",
        "\n",
        "IN_PATH  = \"/Users/19452/Downloads/densho_narrators.csv\"\n",
        "OUT_PATH = \"/Users/19452/Downloads/densho_narrators_clean.csv\"\n",
        "\n",
        "_non_alnum = re.compile(r\"[^A-Za-z0-9\\s]+\")\n",
        "_numbers   = re.compile(r\"\\b\\d+\\b\")\n",
        "_multi_ws  = re.compile(r\"\\s+\")\n",
        "\n",
        "def preview(df, col, n=3):\n",
        "    print(f\"\\n{col}:\")\n",
        "    for i, s in enumerate(df[col].head(n).tolist(), 1):\n",
        "        print(f\"{i}. {s}\")\n",
        "\n",
        "df = pd.read_csv(IN_PATH)\n",
        "df[\"bio\"] = df[\"bio\"].fillna(\"\").astype(str)\n",
        "\n",
        "# (1) Remove noise\n",
        "df[\"bio_no_noise\"] = (\n",
        "    df[\"bio\"].str.replace(_non_alnum, \" \", regex=True)\n",
        "             .str.replace(_multi_ws, \" \", regex=True)\n",
        "             .str.strip()\n",
        ")\n",
        "preview(df, \"bio_no_noise\")\n",
        "\n",
        "# (2) Remove numbers\n",
        "df[\"bio_no_numbers\"] = (\n",
        "    df[\"bio_no_noise\"].str.replace(_numbers, \" \", regex=True)\n",
        "                      .str.replace(_multi_ws, \" \", regex=True)\n",
        "                      .str.strip()\n",
        ")\n",
        "preview(df, \"bio_no_numbers\")\n",
        "\n",
        "# (4) Lowercase\n",
        "df[\"bio_lower\"] = df[\"bio_no_numbers\"].str.lower()\n",
        "preview(df, \"bio_lower\")\n",
        "\n",
        "# (3) Remove stopwords\n",
        "df[\"bio_no_stop\"] = df[\"bio_lower\"].apply(lambda t: \" \".join(w for w in t.split() if w not in STOP))\n",
        "preview(df, \"bio_no_stop\")\n",
        "\n",
        "# (5) Stemming\n",
        "df[\"bio_stemmed\"] = df[\"bio_no_stop\"].apply(lambda t: \" \".join(STEM.stem(w) for w in t.split()))\n",
        "preview(df, \"bio_stemmed\")\n",
        "\n",
        "# (6) Lemmatization\n",
        "if USE_SPACY:\n",
        "    df[\"bio_lemma\"] = df[\"bio_no_stop\"].apply(lambda t: \" \".join(tok.lemma_ for tok in nlp(t)))\n",
        "else:\n",
        "    df[\"bio_lemma\"] = df[\"bio_no_stop\"].apply(lambda t: \" \".join(LEMM.lemmatize(w) for w in t.split()))\n",
        "preview(df, \"bio_lemma\")\n",
        "\n",
        "df.to_csv(OUT_PATH, index=False)\n",
        "print(f\"\\nSaved: {OUT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "outputId": "37369113-eb7e-4e44-ae59-3ba5c82c58d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "(1) POS Counts:\n",
            "Noun: 136\n",
            "Verb: 37\n",
            "Adj: 51\n",
            "Adv: 11\n",
            "\n",
            "(2) Dependency Parse:\n",
            "nisei           amod         bear           \n",
            "female          amod         bear           \n",
            "bear            nsubj        selleck        \n",
            "may             aux          selleck        \n",
            "selleck         ROOT         selleck        \n",
            "washington      dobj         selleck        \n",
            "spend           ccomp        selleck        \n",
            "much            amod         beaverton      \n",
            "childhood       compound     beaverton      \n",
            "beaverton       compound     father         \n",
            "oregon          compound     father         \n",
            "father          nsubj        own            \n",
            "own             xcomp        spend          \n",
            "farm            compound     influence      \n",
            "influence       dobj         own            \n",
            "early           amod         age            \n",
            "age             compound     ii             \n",
            "parent          compound     ii             \n",
            "conversion      compound     ii             \n",
            "christianity    compound     ii             \n",
            "world           compound     ii             \n",
            "war             compound     ii             \n",
            "ii              appos        influence      \n",
            "remove          advcl        selleck        \n",
            "portland        compound     assembly       \n",
            "assembly        compound     center         \n",
            "center          compound     work           \n",
            "oregon          compound     minidoka       \n",
            "minidoka        compound     concentration  \n",
            "concentration   compound     camp           \n",
            "camp            compound     work           \n",
            "idaho           compound     war            \n",
            "war             compound     work           \n",
            "work            nsubj        establish      \n",
            "establish       ccomp        selleck        \n",
            "successful      amod         program        \n",
            "volunteer       compound     program        \n",
            "program         compound     feed           \n",
            "feed            dobj         establish      \n",
            "homeless        compound     seattle        \n",
            "seattle         compound     washington     \n",
            "washington      npadvmod     establish      \n",
            "\n",
            "Example sentence:\n",
            "nisei female bear may selleck washington spend much childhood beaverton oregon father own farm influence early age parent conversion christianity world war ii remove portland assembly center oregon minidoka concentration camp idaho war work establish successful volunteer program feed homeless seattle washington\n",
            "Dependency parsing shows word-to-word relations.\n",
            "Constituency parsing (not in spaCy) shows phrase hierarchy.\n",
            "\n",
            "(3) Named Entities (counts):\n",
            "GPE: 65\n",
            "PERSON: 23\n",
            "DATE: 17\n",
            "NORP: 13\n",
            "ORG: 12\n",
            "EVENT: 8\n",
            "CARDINAL: 2\n",
            "LOC: 2\n",
            "FAC: 1\n",
            "ORDINAL: 1\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import spacy\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True)\n",
        "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
        "\n",
        "# spaCy for parsing + NER\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    raise RuntimeError(\"Run: python -m spacy download en_core_web_sm\")\n",
        "\n",
        "IN_PATH = \"/Users/19452/Downloads/densho_narrators_clean.csv\"\n",
        "\n",
        "df = pd.read_csv(IN_PATH)\n",
        "texts = df[\"bio_lemma\"].dropna().astype(str).tolist()\n",
        "\n",
        "# ---- (1) POS Tagging ----\n",
        "sample = \" \".join(texts[:5])\n",
        "tokens = nltk.word_tokenize(sample)\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "\n",
        "pos_map = {\n",
        "    \"NN\": \"Noun\", \"NNS\": \"Noun\", \"NNP\": \"Noun\", \"NNPS\": \"Noun\",\n",
        "    \"VB\": \"Verb\", \"VBD\": \"Verb\", \"VBG\": \"Verb\", \"VBN\": \"Verb\", \"VBP\": \"Verb\", \"VBZ\": \"Verb\",\n",
        "    \"JJ\": \"Adj\", \"JJR\": \"Adj\", \"JJS\": \"Adj\",\n",
        "    \"RB\": \"Adv\", \"RBR\": \"Adv\", \"RBS\": \"Adv\"\n",
        "}\n",
        "\n",
        "counts = Counter()\n",
        "for _, tag in tagged:\n",
        "    if tag in pos_map:\n",
        "        counts[pos_map[tag]] += 1\n",
        "\n",
        "print(\"\\n(1) POS Counts:\")\n",
        "for k in [\"Noun\", \"Verb\", \"Adj\", \"Adv\"]:\n",
        "    print(f\"{k}: {counts[k]}\")\n",
        "\n",
        "# ---- (2) Parsing ----\n",
        "doc = nlp(texts[0])\n",
        "\n",
        "print(\"\\n(2) Dependency Parse:\")\n",
        "for tok in doc:\n",
        "    print(f\"{tok.text:15} {tok.dep_:12} {tok.head.text:15}\")\n",
        "\n",
        "print(\"\\nExample sentence:\")\n",
        "print(list(doc.sents)[0].text)\n",
        "print(\"Dependency parsing shows word-to-word relations.\")\n",
        "print(\"Constituency parsing (not in spaCy) shows phrase hierarchy.\")\n",
        "\n",
        "# ---- (3) Named Entity Recognition ----\n",
        "all_ents = Counter()\n",
        "for t in texts[:20]:\n",
        "    d = nlp(t)\n",
        "    for ent in d.ents:\n",
        "        all_ents[ent.label_] += 1\n",
        "\n",
        "print(\"\\n(3) Named Entities (counts):\")\n",
        "for ent, c in all_ents.most_common():\n",
        "    print(f\"{ent}: {c}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcVqy1yj3wja"
      },
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEdcyHX8VaDB"
      },
      "source": [
        "#Question 4 (20 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ung5_YW3C6y"
      },
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHubâ€™s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTOfUpatronW"
      },
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dtco9K--ks6",
        "scrolled": true,
        "outputId": "55660825-d138-4f5b-ac27-90eb8a44e792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 1000 rows â†’ /Users/19452/Downloads/github_marketplace_actions.csv\n"
          ]
        }
      ],
      "source": [
        "# Q4 â€“ Part 1: Scrape GitHub Marketplace (Actions) to CSV\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import csv\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "BASE   = \"https://github.com\"\n",
        "URL    = \"https://github.com/marketplace?type=actions&page={}\"\n",
        "OUT    = \"/Users/19452/Downloads/github_marketplace_actions.csv\"\n",
        "FIELDS = [\"product_name\", \"description\", \"url\", \"page\"]\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (INFO5731-Assignment)\"}\n",
        "\n",
        "MAX_ITEMS   = 1000          # target size\n",
        "REQUEST_GAP = 1.0           # polite delay (seconds)\n",
        "TIME_LIMIT  = 15 * 60       # overall cap (seconds)\n",
        "\n",
        "def parse_cards(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    rows = []\n",
        "    cards = soup.select(\"article, li, div.marketplace-item, [data-testid='marketplace-listing-card']\")\n",
        "    for c in cards:\n",
        "        a = c.select_one(\"a[href*='/marketplace/']\") or c.find(\"a\", class_=\"text-bold\") or c.find(\"a\")\n",
        "        if not a or not a.get(\"href\"):\n",
        "            continue\n",
        "        name = a.get_text(strip=True)\n",
        "        href = urljoin(BASE, a[\"href\"])\n",
        "        d = c.select_one(\"p, .color-fg-muted, .marketplace-item-card-desc\")\n",
        "        desc = d.get_text(strip=True) if d else \"\"\n",
        "        if name and href:\n",
        "            rows.append((name, desc, href))\n",
        "    return rows\n",
        "\n",
        "def scrape():\n",
        "    all_rows, start = [], time.time()\n",
        "    for page in range(1, 10000):\n",
        "        if time.time() - start > TIME_LIMIT or len(all_rows) >= MAX_ITEMS:\n",
        "            break\n",
        "        try:\n",
        "            r = requests.get(URL.format(page), headers=HEADERS, timeout=30)\n",
        "            if r.status_code != 200:\n",
        "                break\n",
        "            rows = parse_cards(r.text)\n",
        "            if not rows:\n",
        "                break\n",
        "            for name, desc, href in rows:\n",
        "                all_rows.append({\"product_name\": name, \"description\": desc, \"url\": href, \"page\": page})\n",
        "                if len(all_rows) >= MAX_ITEMS:\n",
        "                    break\n",
        "        except requests.RequestException:\n",
        "            time.sleep(2)\n",
        "            continue\n",
        "        time.sleep(REQUEST_GAP)\n",
        "    return all_rows[:MAX_ITEMS]\n",
        "\n",
        "def main():\n",
        "    data = scrape()\n",
        "    with open(OUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=FIELDS)\n",
        "        w.writeheader()\n",
        "        w.writerows(data)\n",
        "    print(f\"Saved {len(data)} rows â†’ {OUT}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0twhJUhgtJT",
        "outputId": "2053404e-fba9-4f58-f50f-ac93be1d9d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing values per column:\n",
            " product_name            0\n",
            "description          1000\n",
            "url                     0\n",
            "page                    0\n",
            "description_clean       0\n",
            "dtype: int64\n",
            "\n",
            "Duplicate rows: 77\n",
            "\n",
            "Saved â†’ /Users/19452/Downloads/github_marketplace_actions_clean.csv\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# resources\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "\n",
        "IN  = \"/Users/19452/Downloads/github_marketplace_actions.csv\"\n",
        "OUT = \"/Users/19452/Downloads/github_marketplace_actions_clean.csv\"\n",
        "\n",
        "STOP = set(stopwords.words(\"english\"))\n",
        "LEMM = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    if not isinstance(t, str):\n",
        "        return \"\"\n",
        "    t = re.sub(r\"<[^>]+>\", \" \", t)\n",
        "    t = re.sub(r\"[^A-Za-z\\s]\", \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip().lower()\n",
        "    toks = [w for w in nltk.word_tokenize(t) if w not in STOP]\n",
        "    toks = [LEMM.lemmatize(w) for w in toks]\n",
        "    return \" \".join(toks)\n",
        "\n",
        "def main():\n",
        "    df = pd.read_csv(IN)\n",
        "\n",
        "    # preprocess\n",
        "    df[\"description_clean\"] = df[\"description\"].apply(clean_text)\n",
        "\n",
        "    # data quality: report\n",
        "    print(\"\\nMissing values per column:\\n\", df.isna().sum())\n",
        "    print(\"\\nDuplicate rows:\", df.duplicated().sum())\n",
        "\n",
        "    # data quality: fix\n",
        "    df = df.drop_duplicates()\n",
        "    df[\"product_name\"] = df[\"product_name\"].fillna(\"\")\n",
        "    df[\"description\"]   = df[\"description\"].fillna(\"\")\n",
        "    df[\"url\"]           = df[\"url\"].fillna(\"\")\n",
        "    df[\"page\"]          = df[\"page\"].fillna(0).astype(int)\n",
        "\n",
        "    df.to_csv(OUT, index=False)\n",
        "    print(f\"\\nSaved â†’ {OUT}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WeD70ty3Gui"
      },
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "outputId": "1756246a-0e2b-463d-bb84-e32b2f524b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 90 tweets â†’ /Users/19452/Downloads/twitter_ml_ai_raw.csv\n"
          ]
        }
      ],
      "source": [
        "#Q5 â€“ PART 1: Collect tweets (hashtags: ML/AI) â†’ CSV\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "import time, csv, tweepy\n",
        "\n",
        "# paste your keys/tokens\n",
        "API_KEY = \"nUA4bSM2ZIGKL7uTQapXmK6Rw\"\n",
        "API_SECRET = \"SQ9T7Ytz7xrOzVBClRYU7edb0BIzuVZF0PwrNcUpPriywSd4qm\"\n",
        "BEARER = \"AAAAAAAAAAAAAAAAAAAAALm%2F4QEAAAAAESJZ7CuPOIGsqsXEC5wqnxFQw7c%3DETiTZek7V0vrH5iDmBi37PXR5ADWcxfNQ8XAVtoCDP7yS3Ulht\"\n",
        "\n",
        "OUT = \"/Users/19452/Downloads/twitter_ml_ai_raw.csv\"\n",
        "FIELDS = [\"tweet_id\", \"username\", \"text\", \"created_at\"]\n",
        "\n",
        "QUERY = \"(#machinelearning OR #artificialintelligence OR #ml OR #ai) lang:en -is:retweet\"\n",
        "MAX_TWEETS = 90         # stay under your 100 reads/month cap\n",
        "PAGE_DELAY = 1.0\n",
        "\n",
        "def fetch_rows(client, limit):\n",
        "    rows, seen = [], set()\n",
        "    pages = tweepy.Paginator(\n",
        "        client.search_recent_tweets,\n",
        "        query=QUERY,\n",
        "        tweet_fields=[\"id\",\"text\",\"created_at\",\"author_id\",\"lang\"],\n",
        "        user_fields=[\"username\"],\n",
        "        expansions=[\"author_id\"],\n",
        "        max_results=100,\n",
        "    )\n",
        "    for page in pages:\n",
        "        if not page.data: break\n",
        "        users = {u.id: u.username for u in (page.includes.get(\"users\") or [])}\n",
        "        for t in page.data:\n",
        "            if t.lang != \"en\": continue\n",
        "            if t.id in seen:    continue\n",
        "            seen.add(t.id)\n",
        "            rows.append({\n",
        "                \"tweet_id\": str(t.id),\n",
        "                \"username\": users.get(t.author_id, \"\"),\n",
        "                \"text\": (t.text or \"\").replace(\"\\r\",\" \").replace(\"\\n\",\" \").strip(),\n",
        "                \"created_at\": t.created_at.isoformat() if t.created_at else \"\"\n",
        "            })\n",
        "            if len(rows) >= limit: return rows\n",
        "        time.sleep(PAGE_DELAY)\n",
        "    return rows\n",
        "\n",
        "def main():\n",
        "    if not BEARER: raise RuntimeError(\"Add your bearer token.\")\n",
        "    client = tweepy.Client(\n",
        "        bearer_token=BEARER,\n",
        "        consumer_key=API_KEY,\n",
        "        consumer_secret=API_SECRET,\n",
        "        wait_on_rate_limit=True\n",
        "    )\n",
        "    rows = fetch_rows(client, MAX_TWEETS)\n",
        "    with open(OUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=FIELDS)\n",
        "        w.writeheader(); w.writerows(rows)\n",
        "    print(f\"Saved {len(rows)} tweets â†’ {OUT}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvy84_8igtJT",
        "outputId": "34133027-66fa-4851-d556-c40b57f84294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Missing values per column:\n",
            " tweet_id      0\n",
            "username      0\n",
            "text          0\n",
            "created_at    0\n",
            "text_clean    0\n",
            "dtype: int64\n",
            "\n",
            "Duplicate tweet_ids: 0\n",
            "\n",
            "Saved â†’ /Users/19452/Downloads/twitter_ml_ai_clean.csv\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import re, pandas as pd, nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "\n",
        "IN  = \"/Users/19452/Downloads/twitter_ml_ai_raw.csv\"\n",
        "OUT = \"/Users/19452/Downloads/twitter_ml_ai_clean.csv\"\n",
        "\n",
        "STOP = set(stopwords.words(\"english\"))\n",
        "LEMM = WordNetLemmatizer()\n",
        "\n",
        "URL_RE   = re.compile(r\"https?://\\S+\")\n",
        "MENT_RE  = re.compile(r\"@[A-Za-z0-9_]+\")\n",
        "HASH_RE  = re.compile(r\"#\")\n",
        "EMOJI_RE = re.compile(r\"[\\U00010000-\\U0010ffff]\")\n",
        "NON_ALPH = re.compile(r\"[^a-z\\s]\")\n",
        "MULTI_WS = re.compile(r\"\\s+\")\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    if not isinstance(t, str): return \"\"\n",
        "    t = URL_RE.sub(\" \", t)\n",
        "    t = MENT_RE.sub(\" \", t)\n",
        "    t = HASH_RE.sub(\" \", t)\n",
        "    t = EMOJI_RE.sub(\" \", t)\n",
        "    t = t.lower()\n",
        "    t = NON_ALPH.sub(\" \", t)\n",
        "    t = MULTI_WS.sub(\" \", t).strip()\n",
        "    toks = [w for w in nltk.word_tokenize(t) if w not in STOP]\n",
        "    toks = [LEMM.lemmatize(w) for w in toks]\n",
        "    return \" \".join(toks)\n",
        "\n",
        "def main():\n",
        "    df = pd.read_csv(IN, dtype={\"tweet_id\": str}).fillna({\"username\":\"\", \"text\":\"\", \"created_at\":\"\"})\n",
        "    df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "    print(\"\\nMissing values per column:\\n\", df.isna().sum())\n",
        "    print(\"\\nDuplicate tweet_ids:\", df.duplicated(subset=[\"tweet_id\"]).sum())\n",
        "\n",
        "    df = df.drop_duplicates(subset=[\"tweet_id\"])\n",
        "    df.to_csv(OUT, index=False)\n",
        "    print(f\"\\nSaved â†’ {OUT}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8BFCvWp32cf"
      },
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbTa-jDS-KFI"
      },
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}